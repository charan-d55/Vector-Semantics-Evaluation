{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charan-d55/Vector-Semantics-Evaluation/blob/main/Vector_Semantics_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2zAn8TTtfvG",
        "outputId": "5ee34d5a-c8af-41c8-f6c2-5515c3381fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytrec_eval in /usr/local/lib/python3.10/dist-packages (0.5)\n"
          ]
        }
      ],
      "source": [
        "pip install pytrec_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmXZ0Xr7tXGh",
        "outputId": "cc4bcf96-ecc9-4841-ed21-7bce58eab73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data():\n",
        "    brownDS = brown.sents(categories=['news', 'hobbies'])\n",
        "    dd =[]\n",
        "    for i in brownDS:\n",
        "        dd.append(\" \".join(i))\n",
        "    return dd\n",
        "C = data()"
      ],
      "metadata": {
        "id": "oWVjuZ8euC9S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMporting all used libraries"
      ],
      "metadata": {
        "id": "69aOZyuVuV2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import pytrec_eval\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BZ3tb_q_qwBn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown, wordnet\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs"
      ],
      "metadata": {
        "id": "g9xAV3GorBg6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading and storing the gold standard dataset i.e SimLex-999"
      ],
      "metadata": {
        "id": "p74Tb1FwqlDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = data()\n",
        "f = open(\"SimLex-999.txt\", encoding = 'utf-8')\n",
        "W1 = []\n",
        "W2 = []\n",
        "Sim = []\n",
        "for line in f.readlines()[1:]:\n",
        "    fields = line.strip().split('\\t')\n",
        "    W1.append(fields[0].lower())\n",
        "    W2.append(fields[1].lower())\n",
        "    Sim.append(float(fields[3]))\n",
        "f.close()\n",
        "W1 = np.asarray(W1)\n",
        "W2 = np.asarray(W2)\n",
        "Sim = np.asarray(Sim)"
      ],
      "metadata": {
        "id": "cVA5V288qh7W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method for Transitive Analysis"
      ],
      "metadata": {
        "id": "KaOaQ3uitspY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transitive(topk,W1,W2,Sim,vocab):\n",
        "    for word in topk.keys():\n",
        "        if len(topk[word]) >= 10:\n",
        "            topk[word] = sorted(topk[word], reverse=True)[:10]\n",
        "        else:\n",
        "            topk[word] = sorted(topk[word], reverse=True)\n",
        "            tw = np.asarray(topk[word])\n",
        "            addedWords = []\n",
        "            for sim,word2 in topk[word]:\n",
        "                if word2 in W1:\n",
        "                    indices = [i for i, x in enumerate(W1) if x == word2]\n",
        "                    for ind in indices:\n",
        "                        if W2[ind] not in tw[:,1] and W2[ind]!=word and W2[ind] not in addedWords:\n",
        "                            addedWords.append([Sim[ind]*(sim/max(Sim)),W2[ind]])\n",
        "                if word2 in W2:\n",
        "                    indices = [i for i, x in enumerate(W2) if x == word2]\n",
        "                    for ind in indices:\n",
        "                        if W1[ind] not in tw[:,1] and W1[ind]!=word and W1[ind] not in addedWords:\n",
        "                            addedWords.append([Sim[ind]*(sim/max(Sim)),W1[ind]])\n",
        "            addCounter = 0\n",
        "            addedWords = sorted(addedWords, reverse=True)\n",
        "            while len(topk[word])<10 and addCounter<len(addedWords):\n",
        "                topk[word].append(addedWords[addCounter])\n",
        "                addCounter+=1\n",
        "    return topk\n",
        "\n",
        "def transitivityAnalysis(topk,W1,W2,Sim,vocab, render=False):\n",
        "    lens = []\n",
        "    lens.append(notten(topk))\n",
        "    topk = transitive(topk,W1,W2,Sim,vocab)\n",
        "    while notten(topk) != lens[-1]:\n",
        "        lens.append(notten(topk))\n",
        "        topk = transitive(topk,W1,W2,Sim,vocab)\n",
        "    if render:\n",
        "        plt.plot(range(len(lens)),lens)\n",
        "        plt.ylabel('# similar words != 10')\n",
        "        plt.xlabel('# transitive operations')\n",
        "        plt.title('# required transitive operations')\n",
        "        plt.show()\n",
        "    return topk\n",
        "\n",
        "def notten(topk):\n",
        "    count = 0\n",
        "    for i in topk.keys():\n",
        "        if len(topk[i]) != 10:\n",
        "            count+=1\n",
        "    return count"
      ],
      "metadata": {
        "id": "wRxOKpfltrF6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to gather topk in gold standard SimLex-999"
      ],
      "metadata": {
        "id": "1-B0y2L1tKWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def golden_top_K(W1,W2,Sim,vocab):\n",
        "    topk = {}\n",
        "    for i in range(len(W1)):\n",
        "        if W1[i] in vocab:\n",
        "            first = W1[i]\n",
        "            second = W2[i]\n",
        "            if first not in topk.keys():\n",
        "                topk[first] = [[Sim[i], second]]\n",
        "            else:\n",
        "                topk[first].append([Sim[i], second])\n",
        "        if W2[i] in vocab:\n",
        "            first = W2[i]\n",
        "            second = W1[i]\n",
        "            if first not in topk.keys():\n",
        "                topk[first] = [[Sim[i], second]]\n",
        "            else:\n",
        "                topk[first].append([Sim[i], second])\n",
        "    return topk"
      ],
      "metadata": {
        "id": "Y6GC5X3ntI2M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DictonaryGeneration(model,G):\n",
        "    M = {}\n",
        "    for k in model.keys():\n",
        "        M[k] = {}\n",
        "        for v in model[k]:\n",
        "            M[k][v] = 1\n",
        "    GG = {}\n",
        "    for k in G.keys():\n",
        "        GG[k] = {}\n",
        "        for v in G[k]:\n",
        "            GG[k][v[1]] = 1\n",
        "    return M,GG\n",
        "def DictonaryGenerationW2V(model,G):\n",
        "    M = {}\n",
        "    for k in model.keys():\n",
        "        M[k] = {}\n",
        "        for v in model[k]:\n",
        "            M[k][v[0]] = 1\n",
        "    GG = {}\n",
        "    for k in G.keys():\n",
        "        GG[k] = {}\n",
        "        for v in G[k]:\n",
        "            GG[k][v[1]] = 1\n",
        "    return M,GG\n"
      ],
      "metadata": {
        "id": "POV2jRGYum9V"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to gather top 10 similar words in both TF-iDF and Word2Vec"
      ],
      "metadata": {
        "id": "Uffku8aisJ2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(GT, similarities, vocab):\n",
        "    TOPS = {}\n",
        "    vv = np.asarray(vocab)\n",
        "    for i in G.keys():\n",
        "        if i in vv:\n",
        "            ind = vocab.index(i)\n",
        "            simVec = similarities[ind]\n",
        "            inds = np.flip(simVec.argsort()[-11:])\n",
        "            inds = np.setdiff1d(inds,ind)\n",
        "            tops = vv[np.array(inds)]\n",
        "            TOPS[i] = tops\n",
        "    return TOPS\n",
        "def searchW2V(model,G):\n",
        "    TOPS = {}\n",
        "    for i in G.keys():\n",
        "        if model.wv.has_index_for(i):\n",
        "            sims = model.wv.most_similar(i,topn=10)\n",
        "            TOPS[i] = sims\n",
        "    return TOPS\n"
      ],
      "metadata": {
        "id": "fRB3gLLwsIyb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method for evaluation"
      ],
      "metadata": {
        "id": "ogAQcjhDrbkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluatorfunc(model,truth,method):\n",
        "    if method=='TF-iDF':\n",
        "        q, run = DictonaryGeneration(model, truth)\n",
        "    elif method=='Word2Vec':\n",
        "        q, run = DictonaryGenerationW2V(model, truth)\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(q, {f'ndcg'})\n",
        "    a = json.dumps(evaluator.evaluate(run), indent=1)\n",
        "    b = evaluator.evaluate(run)\n",
        "    return b"
      ],
      "metadata": {
        "id": "mBzSf1wXra22"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the value of method variable to TF-iDF or Word2Vec to get mean nDCG"
      ],
      "metadata": {
        "id": "P6gTbw_MrH2S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYTNK7xwt3Mp",
        "outputId": "f86c1bee-dcfd-4f38-d2e3-18fbf115bb24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean ndcg ws:1,vs:10: 0.012483483949684415\n",
            "mean ndcg ws:1,vs:50: 0.012834164991244793\n",
            "mean ndcg ws:1,vs:100: 0.012630419877974391\n",
            "mean ndcg ws:1,vs:300: 0.013412902678666903\n",
            "mean ndcg ws:2,vs:10: 0.010577330790196083\n",
            "mean ndcg ws:2,vs:50: 0.015212929863626686\n",
            "mean ndcg ws:2,vs:100: 0.014905052787661077\n",
            "mean ndcg ws:2,vs:300: 0.013891390469780088\n",
            "mean ndcg ws:5,vs:10: 0.008972266204701613\n",
            "mean ndcg ws:5,vs:50: 0.01786857254310043\n",
            "mean ndcg ws:5,vs:100: 0.018386386898333565\n",
            "mean ndcg ws:5,vs:300: 0.01835527689849163\n",
            "mean ndcg ws:10,vs:10: 0.008113602575895672\n",
            "mean ndcg ws:10,vs:50: 0.01365907585649031\n",
            "mean ndcg ws:10,vs:100: 0.01568093860609313\n",
            "mean ndcg ws:10,vs:300: 0.014134436042879254\n"
          ]
        }
      ],
      "source": [
        "method = 'Word2Vec'\n",
        "if method == 'TF-iDF':\n",
        "    tfIdfVectorizer=tfidf(use_idf=True)\n",
        "    tfIdf = tfIdfVectorizer.fit_transform(C)\n",
        "    d = pd.DataFrame(tfIdf.T.todense())\n",
        "    similarities = cs(d)\n",
        "    keys = tfIdfVectorizer.vocabulary_.keys()\n",
        "    vocab = []\n",
        "    for i in keys:\n",
        "        vocab.append(i)\n",
        "    gg = golden_top_K(W1,W2,Sim,vocab)\n",
        "    G = transitivityAnalysis(copy.deepcopy(gg),W1,W2,Sim,vocab,render=False)\n",
        "    modelTops = search(G, similarities, vocab)\n",
        "    a = evaluatorfunc(modelTops,G,method)\n",
        "    d1 = []\n",
        "    for i in a.keys():\n",
        "        d1.append(a[i]['ndcg'])\n",
        "    print(f'mean ndcg:',sum(d1)/len(d1))\n",
        "\n",
        "elif method == 'Word2Vec':\n",
        "    results = []\n",
        "    window_size = [1, 2, 5, 10]\n",
        "    vector_size = [10, 50, 100, 300]\n",
        "    for ws in window_size:\n",
        "        for vs in vector_size:\n",
        "            model = Word2Vec(sentences=brownDS, vector_size=vs, window=ws, workers=4, epochs=30)\n",
        "            vocab = list(model.wv.index_to_key)\n",
        "            gg = golden_top_K(W1,W2,Sim,vocab)\n",
        "            G = transitivityAnalysis(copy.deepcopy(gg),W1,W2,Sim,vocab)\n",
        "            W2Vtops = searchW2V(model,G)\n",
        "            results = evaluatorfunc(W2Vtops,G,method)\n",
        "            d1 = []\n",
        "            for i in results.keys():\n",
        "                d1.append(results[i]['ndcg'])\n",
        "            print(f'mean ndcg ws:{ws}vs:{vs}:',sum(d1)/len(d1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtX8dQnmKWO2ju0tjBwg0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}